=========================
中文分词
=========================

中文分词是NLP的入门，也是一直研究的热点，出现了各种方法，这里一一道来

最大匹配法
=========================

最简单的分词方法，好像效果也不错，具体又有正向匹配和逆向匹配。

算法步骤是取当前字串在词典中长度最大（前缀/后缀）词作为分词，该算法效果依赖词典，词典可以用hash或trie树结构

http://www.52nlp.cn/maximum-matching-method-of-chinese-word-segmentation


扩展的最大匹配法
==========================

复杂的最大匹配法，主要是根据四条规则改善错误率

核心假设是: The most plausible segmentation is the three-word chunk with maximum length , 翻译为 三词语块（第二和第三词语可以为空）

从当前位置开始找到所有三词语块，如有下面几种可能等：

  _C1_C2_C3C4_

  _C1C2_C3C4_C5_

  _C1C2_C3C4_C5C6_

四条规则：

  1. 最大chunk长度，即三词语块含字符最多的
  2. 最大平均词长，在句末时，第二和第三词可能为空，通过改规则来消除歧义
  3. 最小词长方差， 平均词长一样，选方差小的
  4. 最大单字词语语素自由度之和， 语素自由度可以用频率来表示，出现频率高的单字很可能成词

基于字标注法(Character-based Tagging)
=========================================

基于字标注的分词方法实际上是构词方法。即把分词过程视为字在字串中的标注问题。由于每个字在构造一个特定的词语时都占据着一个确定的构词位置(即词位)

既然基于字标注的中文分词方法是将中文分词当作词性标注的问题来对待，那么就必须有标注对象和标注集了。形象一点，从这个方法的命名上我们就可以推断出它的标注是基本的汉字（还包括一定数量的非汉字字符），而标注集则比较灵活，这些标注集都是依据汉字在汉语词中的位置设计的.，假如规定每个字最多只有四个构词位置：即B(词首)，M(词中)，E(词尾)和S(单独成词)，那么下面句子(甲)的分词结果就可以直接表示成如(乙)所示的逐字标注形式：

(甲)分词结果：／上海／计划／到／本／世纪／末／实现／人均／国内／生产／总值／五千美元／。
(乙)字标注形式：上／B海／E计／g戈lJ／E到／S本／s世／B纪／E末／s实／B现／E人／B均／E国／g内／E生／B产／E总／B值／E五／B千／M美／M元／E。／S


