=========================
中文分词
=========================

中文分词是NLP的入门，也是一直研究的热点，出现了各种方法，这里一一道来

最大匹配法
=========================

最简单的分词方法，好像效果也不错，具体又有正向匹配和逆向匹配。

算法步骤是取当前字串在词典中长度最大（前缀/后缀）词作为分词，该算法效果依赖词典，词典可以用hash或trie树结构

http://www.52nlp.cn/maximum-matching-method-of-chinese-word-segmentation


扩展的最大匹配法
==========================

复杂的最大匹配法，主要是根据四条规则改善错误率

核心假设是: The most plausible segmentation is the three-word chunk with maximum length , 翻译为 三词语块（第二和第三词语可以为空）

从当前位置开始找到所有三词语块，如有下面几种可能等：

  _C1_C2_C3C4_

  _C1C2_C3C4_C5_

  _C1C2_C3C4_C5C6_

四条规则：

  1. 最大chunk长度，即三词语块含字符最多的
  2. 最大平均词长，在句末时，第二和第三词可能为空，通过改规则来消除歧义
  3. 最小词长方差， 平均词长一样，选方差小的
  4. 最大单字词语语素自由度之和， 语素自由度可以用频率来表示，出现频率高的单字很可能成词
