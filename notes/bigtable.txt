Bigtable：分布式结构化数据存储系统
bigtable是一个管理结构化数据的分布式存储系统，它被设计于处理海量数据： 分别于数千通用server上P级别数据。google许多项目是哟个bigtable存储数据，包括网页索引， google地球，google finance。这些应用对bigtable提出需求差异非常大，无论从数据规模（从url到网页到卫星图像）和响应时间（从后台批处理到实时数据服务）。尽管有这些不同的需求， bigtable成功的为他们提供了一个灵活的，高性能的解决方案。在这篇论文中我们将描述bigtable提供的简单数据模型，利用这个模型，客户可以动态控制数据布局和格式。我们还描述了bigtable的设计和实现。
1.介绍
过期两年半时间里，在google我们设计，实现并使用了一个管理结构数据的分布式存储系统，我们称之为bigtable。bigtable设计目的是为可靠支撑p级别数据和数千台机器。bigtable达到如下目标：广泛适用性，可扩展性，高性能和高可用。bigtable被超过60个google产品和项目使用，包括有GA，GF， Orkut， Persionalize Search， Writely和Google Earth。这些产品在多种负载需求情景下使用bigtable，从面向吞吐的批处理到低延时终端用户服务。这些应用使用bigtable集群也有很大差异，从少数到上千台服务器， 最多存储了上百T数据。
从多个方面， bigtable类似数据库：它吸取了许多数据库实现策略。并行数据库和内存数据库有很好扩展性和高性能，但是bigtable提供不同的接口。bigtable不支持完整的关系数据模型，取而代之的是，它为客户提供一种简单数据类型，通过改模型能动态控制数据的布局和格式，并且允许客户推测出在底层存储表现时数据的位置属性。能使用任意string作为行和列名来索引数据。尽管客户经常将各种结构化和半结构化数据序列化成string， bigtable服务端只将数据视为不解释的字符串。客户通过精心选择她们的shema能控制数据的位置。最后，bigtable schema参数使得客户动态控制数据读和写。
2.数据模型
bigtable是一个稀疏，分布式，持久化多维有序map。map通过行，列和一个时间戳索引， map中每个valuea都是不可解释的字节数组。
我们在仔细考虑了bigtable系统潜在用途后选择了这个数据模型。我们先举一个例子，这个例子驱使着我们做出这样的决定，假设我们希望保存大规模网页和相关信息的集合，这个集合会被很多项目使用到，我们叫这个特殊的表为webtable。在webtable中，我们使用url作为row key， 网页的多个属性作为列名，将网页内容存储在contents列，该列使用获取时间戳作为标记。如图一所示。
row
表中的row key是任意字符串（最多至64k，对大部分用户， 10-100字节就够了）。每次读或写单行数据都是原子的（不管这行中有多少列被读取或写入），这个设计决定很容易推出在并发对同一列更新时系统的行为。
bigtable已row key字典序保存数据。表的行被动态分区。每一个分区称为tablet， tablet是数据分布和负载均衡的单位。因此， 读一小段行区间是非常有效并且通常只需与很少机器通信。客户在查询row key时可以利用该特性达到好的本地性。比如， 在webtable中， 通过翻转url hostname使的有相同domain的网页聚集成连续行，如存储maps.google.com/index.html网页数据到key com.google.maps/index.html下。 相同domain网页存储在一起让主机和域名分析更有效。
列簇
列的keys在一起集合中被称为列簇，列簇构成了基本的存取控制单元。在一个列簇中所有数据一般都是同一类型（我们把同一列簇下数据压缩在一起）。列簇必须先被创建，然后数据可在任意关键字下存储在该列簇下。列簇被创建之后，该列簇下任何列关键字都可以被使用。我们计划是，一个表下面不同列簇的个数应该不多（最多几百），列簇在运行中很少被改变。与之成对比的是， 一个表中可能有无数列。
列关键字使用如下命名语法：family:qualifier. 列簇名必须是可打印的， qualifier就可以使任意字符了。一个webtable中实际例子就是language， 其存储了一个页面是使用那种语言写的。 在language簇里我们仅仅使用了一个列关键字， 里面存放了每页language id。表中另一个有用的列簇是anchor，如图一所示，簇中每一列都表示单个anchor。qualifier是引用的站点名， 内容是链接文本。
存取控制和磁盘，内存计数都是在列簇这一个级别。在webtable例子中， 控制允许我们管理服务多个不同应用，有的添加新的原始数据，有的读取原始数据生成新列簇，而有些仅仅只被允许查看现存数据（因为隐私原因甚至可能不能查看所有现存列簇）
时间戳
bigtable中每一数据项对同一数据都可以保存多个版本，版本用时间戳索引。bigtable时间戳是64位整数，时间戳可由bigtable指定，这时代表精确到毫秒级的实时时间，或者也可以由客户端应用程序指定。为了避免冲突，应用程序必须产生唯一的时间戳。同一数据项按时间戳降序存储，所以最新的先被读到。
为了减轻管理多版本数据的负担，我们对每个列簇提供两个设置来告诉bigtable对数据项版本自动垃圾回收。客户端可以指定要么保留最新的n个版本，要么只保留“足够新”的版本。（如只保留最近7天写的数据）。
在我们webtable例子中， 我们在content列里设置被抓取网页的时间戳为该网页实际被抓取时间。用上面介绍的垃圾回收机制使我们只保存最近每个网页3个版本。
api
bigtable api提供创建和删除表与列簇的功能，还提供诸如修改集群，表以及列簇metadata的功能，例如存取控制规则。
客户端应用可以写或删除bigtable中数据，从一单独行或迭代一个表的数据子集来查询数据。图2的c++代码展示了使用RowMutation抽象对象执行一系列更新操作（为使例子简洁，忽略了部分细节）。调用Apply对webtable执行原子修改： 向www.cnn.com增加一个锚，并删除另一个锚。
图3的c++代码展示用一个Scanner抽象对象迭代一个特定行里所有anchor。客户端应用可以跨越迭代多个列簇，有几种机制限制scan输出的行， 列，时间戳。比如，我们可以限制只输出列匹配正则anchor.*.cnn.com的anchors, 或者那些时间戳在当前时间前10天的anchors。
bigtable支持一些特性允许用户以更复杂的方式操纵数据。第一， bigtable支持单行事务，这使得可以在单个行内执行原子的 读-写-改操作。bigtable不支持跨行的事务，尽管bigtable提供接口可以跨行批量修改。第二， bigtable允许把数据项当做整数计数器。最后，bigtable支持在服务器地址空间里执行脚本。脚本是由sawzall（一种google开发用来处理数据的语言）编写。目前基于sawzall的api还不允许客户脚本将数据写入bigtable， 但是他允许客户多种形式的数据转换，基于任意表达式的过滤和通过多种操作符的归纳汇总。
bigtable可以和mapreduce（google开发的支持大规模并行计算框架）一起使用。我们已经开发了一套封装器，允许使用bigtable即可作为mapreduce任务的输入源也可作为其输出目标。
4 构建控件
bigtable是建立在几个google基础设施上的。bigtable使用google分布式文件系统存储log和数据文件。bigtable集群一般允许在一个共享的机器池上，机器上还会运行其他各种各样的分布式应用，bigtable进程和其他应用进程共享机器。bigtable依靠一个集群管理系统来安排任务，在共享机器上管理资源，处理机器故障和监控机器状态。
bigtable数据在内部使用google SSTable文件格式来存储。SSTable提供一个key到valeu持久，有序不可更改映射，key和value都是任意字节串。操作有给定一个key找到相应的value，在一个指定key值范围内遍历key/value对。从内部看，每个SSTable是一个连续数据块（一般是64k，但可以配置）。使用块索引（存储在SSTable最后）来定位数据块，当SSTable打开时，索引被加载到内存中去。一次查询可以通过一次磁盘搜索完成：先对内存中索引用二分查找找到相应的数据块，然后从磁盘中读出该数据块。也可以选择把SSTable完全map到内存，使得搜索和扫描都需要访问磁盘。
bigtable依赖于chubby（一个高可用，持久分布式锁服务组件）。一个chubby服务包括5个活动副本，其中一个被选举出来作为master并积极处理请求。只有在大多数副本运行并能互相通信时服务才是可用的。当有副本失效时，chubby使用paxos算法保证副本的一致性。chubby提供一个命名空间，里面包括目录和一些小文件。每个目录或文件都能被用作锁，读写到一个文件是原子的。chubby客户程序库提供chubby文件一致性缓存。每个chubby客户端维持一个与chubby服务的会话。一个客户会话在不能在租约过期时间内重新续约时会过期。当一个客户会话过期，它失去所有锁和文件句柄。chubby客户程序也可以注册回调函数，在chubby文件和目录通知改变或会话过期时被调用。
bigtable使用chubby完成多种任务：确保在任何时刻有至多一个活动master，存储bigtable数据引导程序位置，发现tablet服务和在tablet服务挂掉之后的善后， 存储bigtable schema信息（每个表的列簇信息）， 存储存取控制列表。如果chubby某段时间变的不可用， bigtable也不可用了。最近我们在跨越11个chubby实例的14个bigtable服务上测试影响。由于chubby不可用（chubby超时或网络问题）导致的bigtable存储数据不可用的bigtable服务时间平均百分比是0.0047%。单个集群由chubby不可用受到虽大影响是0.0326%。
5 实现
bigtable实现主要由三个部分构成：一个连接所有客户应用的库， 一个主服务器和多个tablet服务器。一个集群中可以动态添加或删除一个tablet服务器以适应负载。
master的职责有为tablet服务器分配tablets，监测新加入的和过期的tablet服务器，teblet服务器负载均衡，对GFS中文件进行垃圾回收处理。另外， master还处理schema更改比如创建表和列簇。
每个tablet服务器管理着一个tablets的集合（通常我们每个tablet服务器有10-1000个tablets）。 tablet服务器处理它加载的tablets的读写，以及分割过大的tablets。
如同多数单master分布式存储系统，客户数据移动不通过master：客户直接和tablet服务器通信来读写数据。因为bigtable客户端不需依赖master知道tablet位置信息，绝大多数客户端不与master通信。所以实际上master负载非常轻。
一个bigtable集群中存储了一些数量的表，每个表包含一组tablet，每个tablet包含了某个范围内行的所有数据。初始化时，灭个表只有一个tablet，表增长时，他自动分裂成多个tablets，一般每个tablet大约100-200M大小。
5.1 tablet位置
我们使用三层类似B+树结构来存储tablet位置信息。第一层是存储在chubby中，包含root tablet位置信息的文件。root tablet在一个特殊METADATA表中包含所有表位置信息。每个METADATA tablet包含一组user tablet位置信息。root tablet是METADATA表的第一个表，但是它被特殊对待-它不会被分离，以确保tablet位置信息架构不会多于三层。
METADATA表将每个tablet的位置信息存储于行关键字之下， 行关键字以tablet所在table标示符与tablet最后一行编码而成。每个METADATA行在内存中大约存储1k数据。在一个大小适中，限制在128M 的METADATA tablet，我们的三层位置模式足够寻址2**34个tablets（或128M tablets中可存储2^61字节）
客户端库会缓存tablet位置。如果客户端不知道某个tablet位置，或者它发现缓存的位置信息不对，则它递归的在tablet位置层次上移动。如果客户端缓存是空，定位算法需要三次网络通信，其中包括一次从chubby读数据。如果客户端缓存过期，则定位算法可能需要高达6次网络通信，因为过期缓存条目只有在未查到数据时被发现（假设METADATA tablet不是经常移动）。尽管tabelt位置信息存储在内存中，所以不需要访问GFS，但是，我们通常会在客户端预取tablet信息来减少开销：无论何时读取METADATA表都不只读取一个tablet的元数据。
在METADATA表中我们还存储了次级信息，包含与tabelt有关所有事件日志（比如服务什么时候开启）。这些信息对debug和性能分析很有用。
5.2 Tablet 分配
每个tablet在某个时间只被分配给一个tablet服务器。主服务器跟踪活跃的tablet服务器，当前分配给tablet服务器的tablets，包括那些未分配的。当一个tablet是未分配的，并且一个tablet服务器有足够的空间可用，主服务器通过发送装载请求给tablet服务器来分配这个tablet。
bigtable用chubbt来跟踪记录tablet服务器。当一个tablet服务器启动， 在指定目录下建立一个唯一命名的文件，并获取改文件独占锁。主服务器监视这个目录（服务器目录）来发现tablet服务器。如果tablet服务器丢失了排他锁，比如由于网络断开导致服务器丢失chubby会话，他就停止对tablet提供服务。（chubby提供了有效的机制使得tablet服务器在不拥塞网络情况下保持锁）。只要文件存在，tablet服务器会试图重新获得对其独占锁如果文件不存在，tablet服务器就不再提供服务了，并杀死自己。但一个tablet服务器终止（比如簇管理系统把tablet服务器机器移出簇），它试图释放锁使得主服务更快分配其tabelt。
主服务器职责是监测当一个tablet服务器不能提供服务时，重新尽快分配那些tablet。为了监测tablet服务器何时不再服务其tablet时，主服务器定期轮询每个tablet server锁状态。如果一个tablet服务器报告它的锁丢失，或者master最近几次尝试都无法和服务器通信，主服务器就会试图获得server文件的独占锁。如果主服务器能获得锁，那么说明chubby正常，而tablet服务器要么宕机要么和chubby通信出现故障，所以主服务器通过删除tablet服务器在chubby上的文件确保tablet服务器不再提供服务。一旦服务器的文件被删除，主服务器就把之前分配给该服务器的tablet移动到未分配tablet集合中。为确保bigtable集群master和chubby之间网络不那么脆弱，在chubby会话过期时master会自杀。尽管如此，如上所诉，master节点失效不会改变现有tablet到tablet服务器的分配。
当master由集群管理系统启动时，需要先了解当前tablet分配情况，然后才能改变它。（1）主服务获得在chubby中一个唯一的主锁，代表着当前master实例。（2）master通过扫描chubby上服务目录来发现正在运行服务器（3）master与每个运行的tablet服务器沟通，获得每个服务器被分配的tablet。（4）master扫描METADATA表获得tablet集合。当扫描遇到tablet没有被分配时，master将该tablet加入到未分配tablet，该集合使得tablet有机会被分配。

